def process_file_batch(cursor, batch, files_without_path, files_with_conflicts, files_copied, files_with_multiple_paths):
  # Query the database for each filename and move the file to corresponding path
  for filename in batch:
    cursor.execute("SELECT attachment_path FROM ars_owner.CLAIMATTACHMENTS_VIEW WHERE attachment_name = :attachment_name", [filename])
    results = cursor.fetchall()

    if len(results) ==  1:
      # only proceed if exactly one match is found
      full_path = results[0][0]
      logger.info("Found single path for filename %s", filename)

      relevant_path = extract_relevant_path(full_path)
      if relevant_path is None:
        logger.info("Relevant path not found in full_path %s", full_path)
      else:
        # Split the relevant part into components
        path_parts = relevant_path.split('/')

        # Extract the attachment ID and filename
        attachment_id = path_parts[-2]
        filename = path_parts[-1]

        destination_key = f"attachments/output/{attachment_id}/{filename}"
        destination_path = f"attachments/output/{attachment_id}/"

        move_and_compare_files(bucket_name, f"{folder_name}{filename}", destination_key, destination_path, files_with_conflicts, files_copied)

    elif len(results) > 1:
      # Skip processing if more than one match
      logger.warning("Multiple paths found for filename %s", filename)
      files_with_multiple_paths.append(filename)
    else:
      logger.info("No path found for filename %s", filename)
      files_without_path.append(filename)

def extract_relevant_path(full_path):
  # Normalize the path to handle different formats and slashes
  normalized_path = os.path.normpath(full_path).replace('\\','/')

  #Find the index of relevant directory
  start_index = normalized_path.find('attachments/output')
  if start_index == -1:
    return None

  # Extract the relevant path
  relevant_path = normalized_path[start_index:]
  return relevant_path

def move_and_compare_files(bucket, source_key, destination_key, destination_path, files_with_conflicts, files_copied):
  try:
    # Check if file already exists in the target location
    try:
      s3_client.head_object(Bucket=bucket, Key=destination_key)
      file_exists = True
    except Exception as e:
      file_exists = False

    if file_exists:
      logger.info("File already exists at the destination. checking for byte match: source %s, destination %s",source_key, destination_key)

      existing_file = s3_client.get_object(Bucket=bucket, Key=destination_key)['Body']
      new_file = s3_client.get_object(Bucket=bucket, Key=source_key)['Body']

      # Compare files using direct byte-by-byte comparsion
      if files_are_identical(existing_file, new_file):
        logger.info("Files are identical. Skipping copy: %s %s", source_key, destination_key)
        s3_client.delete_object(Bucket=bucket, Key= source_key)
        logger.info("Deleted the source file,because it is identical to the destination file: %s %s", source_key, destination_key)
      else:
        logger.warning("Files are different, added to the list for manual review: %s %s", source_key, destination_key)
        files_with_conflicts.append(source_key)
        return
    else:
      # Copy the file to the target path if it doesnot exist
      source_bucket_key = {'Bucket': bucket, 'Key': source_key}

      S3TransferUtils.xfer_src_file(t_bucket=bucket_name,
                                      t_prefix=destination_path,
                                      source=source_bucket_key,
                                      delete_src=delete_src)
      logger.info("Moved src file %s, to target %s",source_bucket_key,destination_path)
      files_copied.append(source_key)
  except Exception as e:
    logger.error("Error moving file from source to destination %s", str(e))

def files_are_identical(file1, file2):
  chunk_size = 4096 # Read in chunks 0f 4KB
  while True:
    chunk1 = file1.read(chunk_size)
    chunk2 = file2.read(chunk_size)
    if chunk1 != chunk2:
      return False
    if not chunk1: # End of both files
      return True

def send_email_notification(files_without_path, files_with_conflicts, files_copied, files_with_mulitple_paths, exception_message):
 
  subject = f"{env.upper()} File Processing Report"

  # Construct the email body
  body = "The following issues were encountered during the processing of items for_evaluation."

  if exception_message:
    body += f"An error occurred during file processing:\n\n{exception_message}\n\n"

  if files_without_path:
    body += ("The following files were evaluated but their paths were not found in the database. "
              "Please review these files:\n\n\t" + "\n\t".join(files_without_path) + "\n\n"
             )
  if files_with_mulitple_paths:
    body += ("The following files were skipped due to multiple paths in the database. \n\n\t" +
              "\n\t".join(files_with_mulitple_paths) + "\n\n"
             )
  if files_with_conflicts:
    body += ("The following files require manual review due to difference in content:\n\n\t" +
             "\n\t".join(files_with_conflicts) + "\n\n"
            )
  if files_copied:
    body += ("The following files were successfully moved to their respective locations:\n\n\t" +
              "\n\t".join(files_copied)
             )

  if any([files_without_path, files_with_conflicts, files_copied, files_with_mulitple_paths, exception_message]):
    try:
      SmtpUtils.send_email(source_sender,
                          subject,
                          body,
                          to_addresses)
      logger.info("Email sent successfully %s", to_addresses)
    except Exception as e:
      logger.error("Failed to send email %s", str(e))

def lambda_handler(event,context):
  connection = None
  cursor = None
  files_without_path = [] # List to track the files with no path found
  files_with_conflicts = [] # List to track the files with byte difference
  files_with_multiple_paths = [] # List to track with multiple paths
  files_copied = [] # List to track successfully moved files
  exception_message = None # Track exception messages

  try:
    # List files in the evaluation folder
    files_with_folder = S3TransferUtils.get_s3_versioned_file_list(bucket_name,folder_name)
    files = [os.path.basename(file) for file in files_with_folder]
    files_count = len(files)

    if files_count == 0:
      logger.info("No files found in the bucket: %s folder:%s", bucket_name, folder_name)
      return
    logger.info("Files found in the evaluation folder %s", files)

    dsn = oracledb.makedsn(db_host, db_port, db_sid)

    # Establish connection to oracle database
    connection = oracledb.connect(
        user=db_username,
        password=db_password,
        dsn=dsn)
    logger.debug("connected to oracle database %s", connection)

    # Create a cursor object
    cursor = connection.cursor()

    # Process file in batches
    for i in range(0, len(files), batch_size):
      batch = files[i:i + batch_size]
      process_file_batch(cursor, batch, files_without_path, files_with_conflicts, files_copied, files_with_multiple_paths)

  except oracledb.Error as oe:
    logger.error("Database error: %s", str(oe))
    raise
  except Exception as e:
    logger.error("Error: %s", str(e))
    raise
  finally:
    if cursor is not None:
      cursor.close()
    if connection is not None:
      connection.close()  

    # Send email if there are files without a path or with conflicts or with multiple paths
    send_email_notification(files_without_path, files_with_conflicts, files_copied, files_with_multiple_paths, exception_message )
